{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "#function to remove stop words\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "positive_reviews = BeautifulSoup(open('positive.reviews').read())\n",
    "positive = positive_reviews.findAll('review_text') # review_text is a tag name in the XML file\n",
    "\n",
    "negative_reviews = BeautifulSoup(open('negative.reviews').read())\n",
    "negative = negative_reviews.findAll('review_text') # review_text is a tag name in the XML file\n",
    "\n",
    "# there might be more number of positve reviews\n",
    "# so in order to get equal number of classes for both positive and negative\n",
    "# we are going to first shuffle the positve reviews and cut off them equal to negative reviews count\n",
    "\n",
    "np.random.shuffle(positive)\n",
    "\n",
    "# cut off the positive counts equal to negatives\n",
    "positive = positive[:len(negative)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the below function covers following steps:\n",
    "# 1. removing stop words\n",
    "# 2. tokenizing the words\n",
    "# 3. lemmatizng\n",
    "# 4. convert to lower case\n",
    "\n",
    "def preprocessing(text):\n",
    "    tokens = text.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(tokens)\n",
    "    tokens = [t for t in tokens if len(t) > 2]\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    return tokens\n",
    " \n",
    "word_index_map = {} # creating word indexex for each unique token\n",
    "current_index = 0 # intially Zero and will increase with each token\n",
    "\n",
    "positive_tokens = []# array of positive tokenized words\n",
    "negative_tokens = []# array of negative tokenized words\n",
    "\n",
    "# create a unique word dict for positive reviews ( word: index)\n",
    "for message in positive:\n",
    "    tokens = preprocessing(message.text)\n",
    "    positive_tokens.append(tokens)# add the all the preprocessed tokens to above array\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index +=1 # increment the index value for new token\n",
    "\n",
    "# create a unique word dict for negative reviews ( word: index)\n",
    "for message in negative:\n",
    "    tokens = preprocessing(message.text)\n",
    "    negative_tokens.append(tokens)# add the all the preprocessed tokens to above array\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index +=1 # increment the index value for new token\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ha', 'helped', 'significantly', 'drawing', 'computer', 'program', 'adobe', 'illustrator', 'adobe', 'photoshop', 'macromedia', 'flash', 'drawing', 'mouse', 'like', 'drawing', 'brick', 'buy', 'piece', 'equipment', 'need', 'draw', 'computer', 'also', 'work', 'well', 'overall', 'mouse', 'computer', 'even', 'come', 'mouse', 'prefer', 'use', 'instead', 'pen', 'plug', 'right', 'usb', 'drive', 'work', 'almost', 'instantly']\n",
      "['con', 'tip', 'extremely', 'easy', 'carpet', 'lot', 'cd', 'stacked', 'top', 'poorly', 'designed', 'vertical', 'rack', 'doesnt', 'individual', 'slot', 'cd', 'want', 'bottom', 'stack', 'basically', 'pull', 'whole', 'stack', 'get', 'putting', 'together', 'wa', 'pain', 'one', 'bought', 'break', 'piece', 'metal', 'fit', 'guide', 'hole', 'again..poorly', 'designed', '...', 'doesnt', 'even', 'fit', 'cd', 'well', 'gap', 'ca', 'loose', 'fitting', 'pro', '...', '...', '...', 'guess', 'hold', 'lot', 'cd', '...']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pasted': 11271,\n",
       " 'recourse': 11272,\n",
       " 'cruddy': 11273,\n",
       " 'no-no': 11274,\n",
       " 'volume/on/off': 11275,\n",
       " 'streamlined': 11276,\n",
       " 'slippery': 11277,\n",
       " 'blister': 11278,\n",
       " 'coushing': 11279,\n",
       " 'menetioned': 11280}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing above code\n",
    "print(positive_tokens[0])\n",
    "print(negative_tokens[0])\n",
    "#testing word_index_map dictionary\n",
    "dictionary = {k:word_index_map[k] for k in list(word_index_map)[-10:]}\n",
    "dictionary# token with their index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anudeep\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Rate:  0.84\n"
     ]
    }
   ],
   "source": [
    "# Convert tokens into a data arrays with numbers\n",
    "\n",
    "def token_to_vector(tokens, label): # tokens with lables together to make it easy to shuffle before train them\n",
    "    x = np.zeros(len(word_index_map) + 1) # +1 is for label, so total len(word_index_map+label)\n",
    "    for t in tokens: # loop through each token\n",
    "        i = word_index_map[t] # get the each token index from word_index_map\n",
    "        x[i] += 1 # for each token\n",
    "    #x = x/x.sum()\n",
    "    x[-1] = label\n",
    "    return x\n",
    "\n",
    "N = len(positive_tokens) + len(negative_tokens)\n",
    "# creating a N = 2000 arrays with length of each array equal to len(word_index_map)+1\n",
    "data = np.zeros((N, len(word_index_map) + 1))\n",
    "i = 0\n",
    "for tokens in positive_tokens:\n",
    "    xy = token_to_vector(tokens ,1)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n",
    "for tokens in negative_tokens:\n",
    "    xy = token_to_vector(tokens ,0)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "x = data[:, :-1]\n",
    "y = data[:,-1]\n",
    "\n",
    "X_train = x[:-100, ]\n",
    "y_train = y[:-100, ]\n",
    "X_test = x[-100: , ]\n",
    "y_test = y[-100: , ]\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "print(\"Classification Rate: \",model.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy -0.585213145226766\n",
      "piece -0.5227247977488222\n",
      "even -0.506327464844218\n",
      "almost -0.9459152156763782\n",
      "may -0.5352083957475829\n",
      "two -0.519129495532396\n",
      "using 0.5145291944404375\n",
      "result -0.739435475317519\n",
      "great 1.2023169340047057\n",
      "worth 0.7258537031507079\n",
      "spare 0.6780456238938027\n",
      "description -0.6241466449386905\n",
      "easy 0.8877437050878285\n",
      "radio -0.564120638429105\n",
      "maybe -0.928969838840537\n",
      "pleased 0.6560972647200137\n",
      "bit 0.6706353337641017\n",
      "easier 0.5890360419560926\n",
      "value 0.723050626812834\n",
      "unless -0.765848246579005\n",
      "good 0.51152714406278\n",
      "price 1.0384605515795275\n",
      "fast 1.1001318647074947\n",
      "item -0.7255955058800693\n",
      "last 0.5729288513878242\n",
      "month -0.5132222742837925\n",
      "often -0.8339698070371449\n",
      "big 0.508115982346513\n",
      "lot 0.6074495605973464\n",
      "allows 0.7240916074339978\n",
      "without 0.5235699066772582\n",
      "something -0.5766878784952915\n",
      "minor 0.6206273208852686\n",
      "jack -0.7522943128462749\n",
      "want 0.7930058964474374\n",
      "past 0.5083144291147803\n",
      "quickly 0.580766968241641\n",
      "used 0.6648221773472033\n",
      "highly 1.368429872766604\n",
      "point -0.8980155621041683\n",
      "let 0.5920663809526838\n",
      "away -0.5998518973053264\n",
      "back -1.0436898205684184\n",
      "especially 0.6139491894307443\n",
      "tried -0.6596523600352657\n",
      "excellent 1.4779983915618489\n",
      "thing -0.6589748633113346\n",
      "little 0.5772491109824299\n",
      "cheap -0.7834036442136728\n",
      "handy 0.5735936520781886\n",
      "apple -0.6300276890886074\n",
      "best 1.2601371247468522\n",
      "clear 0.8117385472590467\n",
      "power 0.5097559175815641\n",
      "response -0.556151710838649\n",
      "started -0.7951290352206032\n",
      "hub 0.6364435866578346\n",
      "rebate 0.5529632854788987\n",
      "end -0.7050989056089196\n",
      "output 0.809194120506133\n",
      "charge -0.6390483273277863\n",
      "amazing 0.8209263663521894\n",
      "hour -0.510432451056836\n",
      "died -0.7529946936227301\n",
      "file -0.5704548538332951\n",
      "space 0.7370541385963418\n",
      "installing 0.6608817457741782\n",
      "movie 0.7021318308032595\n",
      "itrip 0.6563341496023133\n",
      "monster -0.5757015613504313\n",
      "decent 0.6447170641160045\n",
      "live -0.9549101269679154\n",
      "job 0.6787772690987852\n",
      "working -0.5855987060614263\n",
      "expected 1.0672278126459682\n",
      "delivery 0.6734254162580546\n",
      "bud -0.6191281478792342\n",
      "comfortable 0.9293156863505753\n",
      "microphone -0.6493969011771039\n",
      "plenty 0.5611869297135348\n",
      "negative 0.6797556177490294\n",
      "extra 0.6780571815407211\n",
      "love 0.9716515304419966\n",
      "said -0.7389925001277249\n",
      "disappointed -0.9955644126931507\n",
      "perfectly 0.8884730637572092\n",
      "true -0.5619313645599123\n",
      "faster 0.6801730578974052\n",
      "gave -0.5567194628809821\n",
      "returned -1.1447363217238746\n",
      "reception 0.6997484957165212\n",
      "plus 0.5353806792813467\n",
      "pretty 0.8167402499715445\n",
      "agree -0.6306941411245648\n",
      "memory 1.0545880765610358\n",
      "advertised 0.6113300989069084\n",
      "perfect 1.4427921913084323\n",
      "outstanding 0.7380732413325088\n",
      "accurate 0.5005138853024524\n",
      "happy 0.7805870803315664\n",
      "simple 0.6324658226578845\n",
      "making -0.546813250184326\n",
      "recording 0.6177360067271467\n",
      "actual -0.5110773650290305\n",
      "waste -1.2947397962637515\n",
      "wrong 0.7166784137683425\n",
      "money -0.748752864077243\n",
      "exactly 0.7859186318478433\n",
      "display -0.6186875740277562\n",
      "top 0.6157838885601714\n",
      "fantastic 0.7277577522984513\n",
      "handle -0.5328721199353705\n",
      "nano 0.5257438264291501\n",
      "warranty -0.5094382078523745\n",
      "investment 0.7114093505093751\n",
      "recorder -0.6211210105208745\n",
      "flimsy -0.5195202869271719\n",
      "deal -0.6034447631016524\n",
      "512 0.8865060531635325\n",
      "tiny -0.6247288819671112\n",
      "worst -0.9545749617848177\n",
      "key -0.5789293009025815\n",
      "figure -0.5048238002152399\n",
      "changed -0.7328180146596642\n",
      "try -0.9205685890332949\n",
      "everyone 0.6247019605915368\n",
      "happened -0.5396113199313018\n",
      "return -1.4688589568005408\n",
      "error -0.5747118524774568\n",
      "useless -0.7449265570911126\n",
      "band -0.604626480651992\n",
      "crisp 0.5533903501347007\n",
      "careful -0.5053934021640492\n",
      "poor -1.2515501094771324\n",
      "bad -0.7240441642898024\n",
      "hope -1.0941040844754057\n",
      "bottom -0.5637000015535502\n",
      "glad 0.6910701376563865\n",
      "fall -0.5335893626893317\n",
      "subwoofer -0.66553308017351\n",
      "terrible -1.2489887166740996\n",
      "worthless -0.652316631672758\n",
      "armband -0.7625233464127882\n",
      "awesome 1.0530954616861057\n",
      "backpack 0.5124169825635683\n",
      "cut -0.5129300035874546\n",
      "pad -0.5334884872954233\n",
      "due -0.6297948245187012\n",
      "kid 0.5004536789694395\n",
      "loved -0.5452503810997067\n",
      "guess -0.5761061848194816\n",
      "thank 0.5071556463882295\n",
      "wrist 0.5963452001834286\n",
      "flaw -1.0352480863245321\n",
      "apart -0.5069840858519123\n",
      "unfortunately -0.8562079012559352\n",
      "nothing -0.5066285882867217\n",
      "care 0.579050875308578\n",
      "suction 0.5006665133510658\n",
      "disappointing -0.5115166879897524\n",
      "air -0.5602354723404673\n",
      "returning -1.0095022596464185\n",
      "label -0.892424503414601\n",
      "whole -0.5128310777153158\n",
      "refund -0.6506803393579836\n",
      "poorly -0.8461077467064445\n",
      "obviously -0.5092207427664391\n",
      "promised -0.6685577502099685\n",
      "thru -0.532129557384544\n",
      "meet -0.6057790670289125\n",
      "ship -0.6116037476344216\n",
      "junk -0.6203601197707205\n",
      "awful -0.548482952598958\n",
      "unusable -0.6742196182189565\n",
      "seemed -0.8056806619618495\n",
      "contract 0.5547530029945448\n",
      "nikon -0.5932931641672129\n",
      "stopped -0.5912585903738231\n",
      "lenmar 0.5068662433332838\n",
      "suck -0.7935758435609828\n",
      "unacceptable -0.5602900393450115\n",
      "coolpix -0.6149130267221398\n",
      "misleading -0.7023051894574622\n",
      "unreliable -0.6986452436231282\n",
      "corrupted -0.5441311746538066\n"
     ]
    }
   ],
   "source": [
    "#lets' see the weights of each token for weights more than the threshold\n",
    "threshold = 0.5\n",
    "for word, index in word_index_map.items():\n",
    "    weight = model.coef_[0][index]# to get the weights\n",
    "    if weight > threshold or weight < -threshold:\n",
    "        print(word, weight)\n",
    "    \n",
    "# -ve weigths represents negativity\n",
    "# +ve weights represents positivity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
